---
layout: default
title: Wang Shida's Personal Website
---
<div class="blurb">
       <h1>Hi, I'm Wang Shida(汪铈达).</h1>
       <p> I'm a fourth-year Ph.D. candidate at math department, National University of Singapore (NUS). <br>
              I'm fortunate to be advised by Professor <a href="https://blog.nus.edu.sg/qianxiaoli/">Li Qianxiao</a>.
              Before that, I studied applied math at Fudan University. <br>
              My research interests are about sequence modelling in machine learning, language models and nonlinear systems. </p>

       <p><em>Educational Background</em>: <br>
              Ph.D. in Mathematics, National University of Singapore, Singapore, 2020-now<br>
              B.S. in Mathematics, Fudan University, Shanghai, China, 2016-2020</p>

       <p><em>Published</em>: <br>
              <a href="https://openreview.net/forum?id=yC2waD70Vj">1. Inverse Approximation Theory for Nonlinear Recurrent Neural Networks (ICLR 2024, spotlight) </a><br>
              &nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/radarFudan/Curse-of-memory">GitHub repo: Curse-of-memory </a><br>
              <a href="http://arxiv.org/abs/2311.14495">2. StableSSM: Alleviating the Curse of Memory in State-space Models through Stable Reparameterization (ICML 2024) </a><br>
              <a href="https://openreview.net/forum?id=i0OmcF14Kf">3. State-space models with layer-wise nonlinearity are universal approximators with exponential decaying memory (NeurIPS 2023) </a><br>
              &nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/radarFudan/Awesome-state-space-models">GitHub repo: Awesome-state-space-models </a><br>
              <a href="https://dl.acm.org/doi/10.1007/978-3-031-43415-0_9">4. Efficient Hyperdimensional Computing (ECML 2023) </a><br>
              <a href="https://www.jml.pub/intro/article_detail/jml/21511.html">5. A Brief Survey on the Approximation Theory for Sequence Modelling (JML 2023)</a><br>
              <a href="https://arxiv.org/abs/2402.13297">6. Integrating Deep Learning and Synthetic Biology: A Co-Design Approach for Enhancing Gene Expression via N-terminal Coding Sequences (Nature Computational Science) </a><br>
       </p>

       <p><em>Manuscripts</em>: <br>
              <a href="https://arxiv.org/abs/2406.02080">1. LongSSM: On the Length Extension of State-space Models in Language Modelling (Submitted) </a><br>
              <a href="https://arxiv.org/abs/2307.11462">2. Improve Long-term Memory Learning Through Rescaling the Error Temporally </a><br>
              <a href="https://arxiv.org/abs/2308.08222">3. HyperSNN: A new efficient and robust deep learning model for resource constrained control applications (Submitted) </a><br>
       </p>

       <p><em>Internships</em>:<br>
              2023.04-2023.12 Sea AI Lab Intern <br>
              2021.08-2021.10 Advance.AI R&D Intern <br>
              2019.07-2020.01 Megvii Intern<br>
              2019.01-2019.02 Goku-data Intern</p>

       <p><em>Travels</em>:<br>
              2024.07.08-2024.07.12 <a href="https://sites.google.com/site/boumedienehamzi/home/fourth-symposium-on-machine-learning-and-dynamical-systems">Symposium on ML and DS</a> <br>
              2024.07.15-2024.07.19 <a href="https://www.scicade2024.org">SciCADE</a> <br>
              2024.07.21-2024.07.27 <a href="https://icml.cc">ICML</a> <br>

       <p><em>TA positions</em>:<br>
              Teaching Assistant for <a href="https://cpb-us-w2.wpmucdn.com/blog.nus.edu.sg/dist/5/11890/files/2021/10/LectureNotes_DSA5102_2021.pdf">DSA5102</a> (2021.08-11)<br>
              Teaching Assistant for DSA5101, DSA5102 (2020.08-11)</p>

       <p><em>Skills</em>: <br>
              Code: Python (PyTorch, JAX, TensorFlow), C/C++, Haskell</p>

       <p><em>Languages</em>: <br>
              Chinese (Mandarin) and English</p>
</div><!-- /.blurb -->
