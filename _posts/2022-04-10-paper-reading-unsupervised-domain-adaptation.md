---
layout: post
title: "paper reading: Connect, Not Collapse: Explaining Contrastive Learning for Unsupervised Domain Adaptation"
date: 2022-04-10
---

## Paper 12

[Connect, Not Collapse: Explaining Contrastive Learning for Unsupervised Domain Adaptation](https://arxiv.org/pdf/2204.00570.pdf)

## What is Contrastive Learning?

## What is Unsupervised Domain Adaptation?

Labeled data from a source domain (e.g., photos) and unlabeled data from a target domain (e.g., sketches) are used to learn a classifier for the target domain.

## Why do we need it?

Conventional UDA methods (e.g., domain adversarial training) learn domain-invariant features to improve generalization to the target domain.



## What's the assumptions of the improvement?



## Contribution of this paper / Main result of this paper?

1. In this paper, we show that contrastive pre-training, which learns features on unlabeled source and target data and then fine-tunes on labeled source data, is competitive with strong UDA methods.

2.

## Reference
