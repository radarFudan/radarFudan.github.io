---
layout: post
title: "paper reading: How reservoir computing present the approximation capacity of neural network"
date: 2022-10-22
---

## Paper 13-15

[A learning result for continuous-time recurrent neural networks](http://www.sontaglab.org/FTPDIR/recur-learn.pdf)

1998

[Reservoir computing approaches to recurrent neural network training](https://amygdala.psychdept.arizona.edu/CompNeuro/Readings/week13/Lukosevicius-Jaeger+Reservoir-computing-recurrent-neural-network+CompSciRev+2019.pdf)

2009:
This review systematically surveys both current ways of generating/adapting the reservoirs and training different types of readouts.

Thus Liquid State Machines use more sophisticated and biologically realistic models of spiking integrate-and-fire neurons and dynamic synaptic connection models in the reservoir.


[paper name]()

## Reference

